{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from pypylon import pylon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaslerEnv(Env):\n",
    "    \"\"\"A goal-based environment. It functions just as any regular OpenAI Gym environment but it\n",
    "    imposes a required structure on the observation_space. More concretely, the observation\n",
    "    space is required to contain at least three elements, namely `observation`, `desired_goal`, and\n",
    "    `achieved_goal`. Here, `desired_goal` specifies the goal that the agent should attempt to achieve.\n",
    "    `achieved_goal` is the goal that it currently achieved instead. `observation` contains the\n",
    "    actual observations of the environment as per usual.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold = -24):\n",
    "        super(BaslerEnv, self).__init__()\n",
    "        \n",
    "        # Create an instant camera object with the camera device found first.\n",
    "        self.camera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())\n",
    "        self.camera.Open()\n",
    "        \n",
    "        self.width = self.camera.Width.GetValue()\n",
    "        self.height = self.camera.Height.GetValue()\n",
    "        \n",
    "        self.image = None\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.action_space = spaces.Dict({'Gain': spaces.Box(low = 36, \n",
    "                                                            high = 512, \n",
    "                                                            shape = (1,), \n",
    "                                                            dtype = np.int64),\n",
    "                                        'Exposure_Time': spaces.Box(low = 24, \n",
    "                                                                    high = 10e7, \n",
    "                                                                    shape = (1,),\n",
    "                                                                    dtype = np.int64),\n",
    "                                        'Acquisition_Frame_Rate': spaces.Box(low = 0, \n",
    "                                                                             high = 10e1, \n",
    "                                                                             shape = (1,), \n",
    "                                                                             dtype = np.int64)\n",
    "                                        })\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=255, \n",
    "                                             shape=(self.height, self.width, 3), \n",
    "                                             dtype=np.float16)\n",
    "        \n",
    "        img = pylon.PylonImage()\n",
    "        self.converter = pylon.ImageFormatConverter()\n",
    "        # converting to opencv bgr format\n",
    "        self.converter.OutputPixelFormat = pylon.PixelType_BGR8packed\n",
    "        self.converter.OutputBitAlignment = pylon.OutputBitAlignment_MsbAligned\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        # Enforce that each GoalEnv uses a Goal-compatible observation space.\n",
    "        if not isinstance(self.observation_space, gym.spaces.Dict):\n",
    "            raise error.Error('GoalEnv requires an observation space of type gym.spaces.Dict')\n",
    "        for key in ['observation', 'achieved_goal', 'desired_goal']:\n",
    "            if key not in self.observation_space.spaces:\n",
    "                raise error.Error('GoalEnv requires the \"{}\" key to be part of the observation dictionary.'.format(key))\n",
    "\n",
    "    def compute_reward(self, image, size=60):\n",
    "        \"\"\"Compute the step reward. This externalizes the reward function and makes\n",
    "        it dependent on a desired goal and the one that was achieved. If you wish to include\n",
    "        additional rewards that are independent of the goal, you can include the necessary values\n",
    "        to derive it in 'info' and compute it accordingly.\n",
    "        Args:\n",
    "            achieved_goal (object): the goal that was achieved during execution\n",
    "            desired_goal (object): the desired goal that we asked the agent to attempt to achieve\n",
    "            info (dict): an info dictionary with additional information\n",
    "        Returns:\n",
    "            float: The reward that corresponds to the provided achieved goal w.r.t. to the desired\n",
    "            goal. Note that the following should always hold true:\n",
    "                ob, reward, done, info = env.step()\n",
    "                assert reward == env.compute_reward(ob['achieved_goal'], ob['goal'], info)\n",
    "        \"\"\"\n",
    "        # grab the dimensions of the image and use the dimensions to\n",
    "        # derive the center (x, y)-coordinates\n",
    "        (h, w, m) = image.shape\n",
    "        (cX, cY) = (int(w / 2.0), int(h / 2.0))\n",
    "\n",
    "        fft = np.fft.fft2(image)\n",
    "        fftShift = np.fft.fftshift(fft)\n",
    "\n",
    "        # zero-out the center of the FFT shift (i.e., remove low\n",
    "        # frequencies), apply the inverse shift such that the DC\n",
    "        # component once again becomes the top-left, and then apply\n",
    "        # the inverse FFT\n",
    "        fftShift[cY - size:cY + size, cX - size:cX + size] = 0\n",
    "        fftShift = np.fft.ifftshift(fftShift)\n",
    "        recon = np.fft.ifft2(fftShift)\n",
    "\n",
    "        # compute the magnitude spectrum of the reconstructed image,\n",
    "        # then compute the mean of the magnitude values\n",
    "        magnitude = 20 * np.log(np.abs(recon))\n",
    "        mean = np.mean(magnitude)\n",
    "        # the image will be considered \"blurry\" if the mean value of the\n",
    "        # magnitudes is less than the threshold value\n",
    "        return mean\n",
    "    \n",
    "    def grab_image(self, action):\n",
    "        gain, time, rate = action['Gain'], action['Exposure_Time'], action['Acquisition_Frame_Rate']\n",
    "        \n",
    "        self.camera.GainRaw.SetValue(int(gain))\n",
    "        self.camera.ExposureTimeRaw.SetValue(int(time))\n",
    "        self.camera.AcquisitionFrameRateAbs.SetValue(int(rate))\n",
    "        \n",
    "        self.camera.StartGrabbing()\n",
    "        while 1:\n",
    "            grabResult = self.camera.RetrieveResult(5000, pylon.TimeoutHandling_ThrowException)\n",
    "\n",
    "            if grabResult.GrabSucceeded():\n",
    "                # Access the image data\n",
    "                self.image = converter.Convert(grabResult)\n",
    "                self.image = self.image.GetArray()\n",
    "#                 print(img[0])\n",
    "#                 print('shape:', img.shape)\n",
    "#                 plt.imshow(img)\n",
    "                break\n",
    "    \n",
    "        self.camera.StopGrabbing()\n",
    "        return self.image\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        obs = self.grab_image(action)\n",
    "        blurry = self.compute_reward(obs)\n",
    "\n",
    "        done = bool(blurry < self.threshold)\n",
    "        reward = -1\n",
    "\n",
    "        return obs, reward, done, {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
