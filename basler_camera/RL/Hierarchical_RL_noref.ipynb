{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\cig\\anaconda3\\envs\\rlautofocus\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.spaces import Box, Discrete, Tuple, MultiDiscrete\n",
    "import logging\n",
    "import random\n",
    "from PIL import Image\n",
    "from pypylon import pylon\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "import ray\n",
    "from ctypes import *\n",
    "import os\n",
    "from ray import tune\n",
    "from ray.tune import function\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTFPolicy, PPOTorchPolicy\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error\n",
    "from gym.utils import closer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ray.tune.logger import pretty_print\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from gym import error\n",
    "from gym.utils import closer\n",
    "from gym import spaces\n",
    "import time\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "import imquality.brisque as brisque\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera_Env(gym.Env):\n",
    "    def __init__(self, camera, \n",
    "                       Width = 417,\n",
    "                       Height = 404):\n",
    "        \n",
    "        self.camera = camera\n",
    "        self.camera.AcquisitionFrameRateAbs.Value = 10\n",
    "        self.camera.GainRaw.Value = 36\n",
    "        self.camera.AcquisitionMode.SetValue('Continuous')\n",
    "        self.Width = Width\n",
    "        self.Height = Height\n",
    "        self.camera.Width.Value = Width\n",
    "        self.camera.Height.Value = Height\n",
    "        print(self.camera.Width.GetValue(), self.camera.Height.GetValue())\n",
    "        # see https://github.com/basler/pypylon/blob/master/samples/opencv.py\n",
    "        img = pylon.PylonImage()\n",
    "        self.converter = pylon.ImageFormatConverter()\n",
    "        # converting to opencv bgr format\n",
    "        self.converter.OutputPixelFormat = pylon.PixelType_BGR8packed\n",
    "        self.converter.OutputBitAlignment = pylon.OutputBitAlignment_MsbAligned\n",
    "        \n",
    "        \n",
    "        self.range_ex_time = np.concatenate((np.arange(1e3, 45e3, 1000), \n",
    "                                             np.arange(50e3, 500e3, 5000), \n",
    "                                             np.arange(600e3, 900e3, 50000), \n",
    "                                             np.arange(100e4, 500e4, 500000))).astype(np.int64)\n",
    "        \n",
    "        number_of_actions = len(self.range_ex_time)\n",
    "        print(number_of_actions)\n",
    "        \n",
    "        self.action_space = Discrete(number_of_actions)\n",
    "        # image\n",
    "        self.observation_space = Box(0, 1, (2048,))#(len(np.zeros((404, 417, 3)).flatten()),))\n",
    "#         self.observation_space = Box(low=0, \n",
    "#                                     high=255, \n",
    "#                                     shape=(Height, Width, 3),\n",
    "#                                     dtype = np.uint8)\n",
    "        \n",
    "        print('---------------camera env init-------------------')\n",
    "        \n",
    "    def grab_image(self):\n",
    "        # see https://github.com/basler/pypylon/blob/master/samples/opencv.py\n",
    "        self.camera.StartGrabbing()\n",
    "        while 1:\n",
    "            grabResult = self.camera.RetrieveResult(5000, pylon.TimeoutHandling_ThrowException)\n",
    "            if grabResult.GrabSucceeded():\n",
    "                # Access the image data\n",
    "                image = self.converter.Convert(grabResult)\n",
    "                image = image.GetArray()\n",
    "                break\n",
    "        self.camera.StopGrabbing()\n",
    "        return image\n",
    "    \n",
    "    def basler(self, action):  \n",
    "        ExposureTimeRaw = int(self.range_ex_time[action])\n",
    "        print('ExposureTimeRaw:', ExposureTimeRaw) \n",
    "        self.camera.ExposureTimeRaw.Value = ExposureTimeRaw\n",
    "        time.sleep(3) \n",
    "        \n",
    "    def check_hist(self, img):\n",
    "        #print('check img')\n",
    "        hist , bin_edges = np.histogram(np.array(img).ravel())\n",
    "        max_value_bin = bin_edges[np.argmax(hist)]\n",
    "        if max_value_bin > 50 and max_value_bin < 150:\n",
    "            pic_ok = True\n",
    "        else:\n",
    "            pic_ok = False\n",
    "        return pic_ok\n",
    "    \n",
    "    def reset(self):\n",
    "        self.camera.ExposureTimeRaw.Value = 20000\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.basler(action)\n",
    "#         self.lens_environment.lens_movement(46.0)\n",
    "        img = self.grab_image()\n",
    "        done = self.check_hist(img)\n",
    "        if done:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        return img, reward, done, {}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lens_Env(gym.Env):\n",
    "    def __init__(self, camera,\n",
    "                        Height = 404, \n",
    "                         Width = 417):\n",
    "        \n",
    "        # corning lib\n",
    "        self.lib = cdll.LoadLibrary(r\"C:\\Users\\CIG\\Documents\\MATLAB\\ComCasp64.dll\")\n",
    "        #Check if Maxim driver dll is loaded\n",
    "        eCOMCaspErr = getattr(self.lib,'Casp_OpenCOM')\n",
    "        print('eCOMCaspErr:', eCOMCaspErr(), self.lib.Casp_OpenCOM())\n",
    "        \n",
    "        self.Width = Width\n",
    "        self.Height = Height\n",
    "        \n",
    "        self.action_space = MultiDiscrete([69, 99])\n",
    "        # image\n",
    "        self.observation_space = Box(0, 1, (2048, ))#(len(np.zeros((404, 417, 3)).flatten()),))\n",
    "#         self.observation_space = Box(low=0, \n",
    "#                                             high=255, \n",
    "#                                             shape=(self.Height, self.Width, 3),\n",
    "#                                             dtype = np.uint8)\n",
    "        self.camera_env = Camera_Env(camera)\n",
    "        print('---------------lens env init-------------------')\n",
    "    \n",
    "    def lens_movement(self, action):\n",
    "        x = c_double(action)\n",
    "        self.lib.Casp_SetFocusVoltage(x)\n",
    "        time.sleep(3)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.lens_movement(46.0)\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.lens_movement(action)\n",
    "        img = self.camera_env.grab_image()\n",
    "        return img, -1, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaslerEnv(MultiAgentEnv):\n",
    "    def __init__(self, \n",
    "                       Width = 417,\n",
    "                       Height = 404,\n",
    "                       threshold = 30):\n",
    "        \n",
    "        self.camera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())\n",
    "        self.camera.Open()\n",
    "\n",
    "        self.basler_env = Camera_Env(self.camera)\n",
    "        self.corning_env = Lens_Env(self.camera)\n",
    "        \n",
    "        # yolo\n",
    "        self.model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', \n",
    "                                         force_reload=True, \n",
    "                                         pretrained=True)\n",
    "        \n",
    "        self.resnet152 = models.resnet152(pretrained=True)\n",
    "        modules=list(self.resnet152.children())[:-1]\n",
    "        self.resnet152=nn.Sequential(*modules)\n",
    "        for p in self.resnet152.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        print(True)\n",
    "        self.histogram_done = False\n",
    "        \n",
    "#         column_names = [\"Ex_time\", \"lens\", \"done\"]\n",
    "\n",
    "        #file_name = \"rl\" + \".csv\"\n",
    "#         file_ = open(r'D:\\RL_autofocus\\rl.csv', \"w\")\n",
    "#         writer = csv.writer(file_, delimiter = \",\")\n",
    "#         self.writer = writer\n",
    "#         writer.writerow(column_names)\n",
    "        \n",
    "        \n",
    "    def take_features(self, img):\n",
    "        preprocess = transforms.Compose([\n",
    "                            transforms.Resize(256),\n",
    "                            transforms.CenterCrop(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "        \n",
    "        image = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "        input_tensor = preprocess(image)\n",
    "        input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "        \n",
    "        # Run through the convolutional layers and resize the output.\n",
    "        features_output = self.resnet152(input_batch)\n",
    "        classifier_input = features_output.view(1, -1)\n",
    "        \n",
    "        return classifier_input.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def reset(self):\n",
    "\n",
    "        self.basler_env.reset()\n",
    "        self.corning_env.reset()\n",
    "        \n",
    "        self.steps_remaining_at_level = None\n",
    "        self.num_high_level_steps = 0\n",
    "        \n",
    "        self.low_level_agent_id = \"low_level_{}\".format(self.num_high_level_steps)\n",
    "        \n",
    "        obs = self.basler_env.grab_image()\n",
    "        feature_obs = self.take_features(obs)\n",
    "        self.histogram_done = False\n",
    "        \n",
    "#         im = Image.fromarray(obs)\n",
    "#         filename = self.filepath + '\\Start' + \"\\img_%s.png\" % (str(self.n))\n",
    "#         im.save(filename)\n",
    "        \n",
    "        return {\"high_level_agent\": feature_obs,}\n",
    "    \n",
    "    def find_class(self, results, obj = 'car'):\n",
    "        len_of_class = len(results.pred)\n",
    "        pred_class = [int(results.pred[0][i].numpy()[5]) for i in range(len(results.pred[0]))]\n",
    "        #list_of_yolo_classes = list(zip(results.names, range(0, len(results.names))))\n",
    "        pred_names = [results.names[i] for i in pred_class]\n",
    "        if obj in pred_names:\n",
    "            index = pred_names.index(obj)\n",
    "            return results.pred[0][index].numpy()\n",
    "        else:\n",
    "            return np.array([])\n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        if \"high_level_agent\" in action_dict:\n",
    "            return self._high_level_step(action_dict[\"high_level_agent\"])\n",
    "        else:\n",
    "            return self._low_level_step(list(action_dict.values())[0])\n",
    "    \n",
    "    \n",
    "    def _high_level_step(self, action):\n",
    "        self.histogram_done = False\n",
    "        self.action_main = action\n",
    "        \n",
    "#         logger.debug(\"High level agent action\".format(action))\n",
    "        \n",
    "        #self.obs_camera, reward_camera, done_c, _ = self.basler_env.step(action)\n",
    "        self.basler_env.basler(action)\n",
    "        #self.corning_env.lens_movement(46.0)\n",
    "        self.obs_camera = self.basler_env.grab_image()\n",
    "        \n",
    "        self.steps_remaining_at_level = 10\n",
    "        self.num_high_level_steps += 1\n",
    "        self.low_level_agent_id = \"low_level_{}\".format(self.num_high_level_steps)\n",
    "        \n",
    "        feature_obs = self.take_features(self.obs_camera)\n",
    "        obs = {self.low_level_agent_id: feature_obs}\n",
    "        rew = {self.low_level_agent_id: 0}\n",
    "        \n",
    "        done = {\"__all__\": False}\n",
    "        \n",
    "        return obs, rew, done, {}\n",
    "    \n",
    "    def _low_level_step(self, action):\n",
    "        \n",
    "        action_lens_coarse = 0\n",
    "        action_lens_fine = action[1]\n",
    "        if action[0] < 24:\n",
    "            action_lens_coarse = 24\n",
    "        else:\n",
    "            action_lens_coarse = action[0]\n",
    "        action_lens = float(\"%s.%s\"% (str(action_lens_coarse), str(action_lens_fine)))\n",
    "        \n",
    "        self.corning_env.lens_movement(action_lens)\n",
    "        obs_lens = self.basler_env.grab_image()\n",
    "        feature_obs = self.take_features(obs_lens)\n",
    "        obs = {self.low_level_agent_id: feature_obs}\n",
    "        if self.histogram_done == False:\n",
    "            self.histogram_done = self.basler_env.check_hist(obs_lens)\n",
    "        \n",
    "#         logger.debug(\"Low level agent step {}\".format(action_lens))\n",
    "        done = {\"__all__\": False}\n",
    "        flag = False\n",
    "        if self.histogram_done == True:\n",
    "            self.steps_remaining_at_level -= 1\n",
    "            \n",
    "            prediction_yolo = self.model_yolo(obs_lens)\n",
    "            final_result_yolo = self.find_class(prediction_yolo)\n",
    "            \n",
    "            if final_result_yolo.size == 0:\n",
    "                rew = {self.low_level_agent_id: -1}\n",
    "                if self.steps_remaining_at_level == 0:\n",
    "                    done[self.low_level_agent_id] = True\n",
    "                    rew[\"high_level_agent\"] = 0.5\n",
    "                    obs[\"high_level_agent\"] = feature_obs\n",
    "            \n",
    "            else:\n",
    "                flag = True\n",
    "                image = Image.fromarray(obs_lens)\n",
    "                image = image.crop((final_result_yolo[0], \n",
    "                                 final_result_yolo[1], \n",
    "                                 final_result_yolo[2], \n",
    "                                 final_result_yolo[3]))\n",
    "                brisque_score = round(brisque.score(image), 2)\n",
    "                \n",
    "                reward = - 0.01*brisque_score\n",
    "                \n",
    "                done[\"__all__\"] = True\n",
    "                rew = {self.low_level_agent_id: reward}\n",
    "                rew[\"high_level_agent\"] = 1\n",
    "                obs[\"high_level_agent\"] = feature_obs\n",
    "                \n",
    "                im = Image.fromarray(obs_lens)\n",
    "                filename = r'D:' + \"\\img_%s_%s_%s.png\" % (str(self.action_main),\n",
    "                                                          str(action_lens), str(brisque_score))\n",
    "                im.save(filename)\n",
    "\n",
    "                \n",
    "        else:\n",
    "            rew = {self.low_level_agent_id: 0}\n",
    "            done[self.low_level_agent_id] = True\n",
    "            rew[\"high_level_agent\"] = -10\n",
    "            obs[\"high_level_agent\"] = feature_obs\n",
    "            \n",
    "        im = Image.fromarray(obs_lens)\n",
    "        filename = r'D:' + \"\\img_%s_%s_%s.png\" % (str(self.action_main),\n",
    "                                                      str(action_lens), str(flag))\n",
    "        im.save(filename)\n",
    "        \n",
    "        print('action:', action, 'info:',  done, rew)\n",
    "        #self.writer.writerow([self.action_main, action_lens, flag])\n",
    "        return obs, rew, done, {}\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return  BaslerEnv(env_config)\n",
    "\n",
    "register_env(\"BaslerEnv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instant camera object with the camera device found first.\n",
    "# camera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())\n",
    "# camera.Open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray import tune\n",
    "from ray.tune import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.rllib.agents.impala.vtrace_tf_policy import VTraceTFPolicy\n",
    "# from ray.rllib.agents.impala import ImpalaTrainer\n",
    "# import ray.rllib.agents.impala as impala\n",
    "\n",
    "# ray.init()\n",
    "\n",
    "# def policy_mapping_fn(agent_id):\n",
    "#     if agent_id.startswith(\"low_level_\"):\n",
    "#         return \"low_level_policy\"\n",
    "#     else:\n",
    "#         return \"high_level_policy\"\n",
    "    \n",
    "    \n",
    "# stop = {\"training_iteration\": 2000,\n",
    "#         \"timesteps_total\": 1000,\n",
    "#         \"episode_reward_mean\": 0.0,}\n",
    "\n",
    "# # policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, \n",
    "# # learning_rate=0.00025, vf_coef=0.5, \n",
    "# # max_grad_norm=0.5, lam=0.95, \n",
    "# # nminibatches=4, noptepochs=4, \n",
    "# # cliprange=0.2, \n",
    "# # cliprange_vf=None, \n",
    "# # verbose=0, tensorboard_log=None, \n",
    "# # _init_setup_model=True, \n",
    "# # policy_kwargs=None, \n",
    "# # full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None\n",
    "\n",
    "\n",
    "# config = {\n",
    "#     \"env\": BaslerEnv,\n",
    "#     \"num_workers\": 1,\n",
    "#     \"entropy_coeff\": 0.01,\n",
    "#     #\"rollout_fragment_length\": 128,\n",
    "#     \"train_batch_size\": 128,\n",
    "#     \"learner_queue_timeout\": 1600,\n",
    "#     #\"sgd_minibatch_size\": 128,\n",
    "#     \"lr\": 0.00025,#tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "#     \"multiagent\": {\n",
    "#         \"policies\": {\n",
    "            \n",
    "#             \"high_level_policy\": (VTraceTFPolicy, \n",
    "#                                   Box(0, 255, (2048,)),#(len(np.zeros((404, 417, 3)).flatten()),)),\n",
    "#                                   Discrete(148), \n",
    "#                                   {\"gamma\": 0.99}),\n",
    "            \n",
    "#             \"low_level_policy\": (VTraceTFPolicy,\n",
    "#                                  Box(0, 255, (2048,)),#(len(np.zeros((404, 417, 3)).flatten()),)),\n",
    "#                                  MultiDiscrete([69, 99]), \n",
    "#                                  {\"gamma\": 0.0}),\n",
    "            \n",
    "# #             \"low_level_policy\": (PPOTFPolicy,\n",
    "# #                                  Tuple([\n",
    "# #                                      lens_env.observation_space,\n",
    "# #                                      Discrete(148)\n",
    "# #                                  ]), maze.action_space, {\n",
    "# #                                      \"gamma\": 0.0\n",
    "# #                                  }),\n",
    "#         },\n",
    "#         \"policy_mapping_fn\": function(policy_mapping_fn),\n",
    "#     },\n",
    "#     \"framework\": \"tf\",#\"torch\", #if args.torch else \"tf\",\n",
    "#     # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "#     \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "# }\n",
    "\n",
    "# #from ray.rllib.agents.trainer_template import build_trainer\n",
    "\n",
    "# # <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "# # MyTrainer = build_trainer(\n",
    "# #     name=\"MyCustomTrainer\",\n",
    "# #     default_policy=MyTFPolicy)\n",
    "\n",
    "# # ray.init()\n",
    "# # tune.run(MyTrainer\n",
    "# trainer = impala.ImpalaTrainer(env= \"BaslerEnv\", config=config)\n",
    "# results = tune.run(\"PPO\", config=config, stop=stop,  \n",
    "#                    restore=r'C:\\Users\\CIG\\Documents\\MATLAB\\ppo_pic\\checkpoint-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ModelCatalog.register_custom_model(\"PPO_2\",PPO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(np.zeros((404, 417, 3)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 16:28:21,523\tINFO services.py:1174 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-04-27 16:28:23,945\tWARNING sample.py:404 -- DeprecationWarning: wrapping <function policy_mapping_fn at 0x000001FFB404EAF8> with tune.function() is no longer needed\n",
      "2021-04-27 16:28:23,993\tINFO trainer.py:616 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-04-27 16:28:23,993\tINFO trainer.py:643 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m WARNING:tensorflow:From c:\\users\\cig\\anaconda3\\envs\\rlautofocus\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 417 404\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 148\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ---------------camera env init-------------------\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m eCOMCaspErr: 0 0\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 417 404\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 148\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ---------------camera env init-------------------\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ---------------lens env init-------------------\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\CIG/.cache\\torch\\hub\\master.zip\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m                  from  n    params  module                                  arguments                     \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m  24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPS\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m Adding autoShape... \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m YOLOv5  2021-4-27 torch 1.8.0+cpu CPU\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m True\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 2021-04-27 16:28:33,786\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 2021-04-27 16:28:34,730\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-27 16:28:36,325\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-27 16:28:37,278\tWARNING deprecation.py:34 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "2021-04-27 16:28:42,918\tINFO trainable.py:103 -- Trainable.setup took 18.928 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-04-27 16:28:42,918\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    if agent_id.startswith(\"low_level_\"):\n",
    "        return \"low_level_policy\"\n",
    "    else:\n",
    "        return \"high_level_policy\"\n",
    "    \n",
    "    \n",
    "stop = {\"training_iteration\": 2000,\n",
    "        \"timesteps_total\": 1000,\n",
    "        \"episode_reward_mean\": 0.0,}\n",
    "\n",
    "# policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, \n",
    "# learning_rate=0.00025, vf_coef=0.5, \n",
    "# max_grad_norm=0.5, lam=0.95, \n",
    "# nminibatches=4, noptepochs=4, \n",
    "# cliprange=0.2, \n",
    "# cliprange_vf=None, \n",
    "# verbose=0, tensorboard_log=None, \n",
    "# _init_setup_model=True, \n",
    "# policy_kwargs=None, \n",
    "# full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"env\": BaslerEnv,\n",
    "    \"num_workers\": 1,\n",
    "    \"entropy_coeff\": 0.01,\n",
    "    \"rollout_fragment_length\": 128,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"sgd_minibatch_size\": 128,\n",
    "    \"lr\": 0.00025,#tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \n",
    "            \"high_level_policy\": (PPOTFPolicy, \n",
    "                                  Box(0, 255, (2048,)),#(len(np.zeros((404, 417, 3)).flatten()),)),\n",
    "                                  Discrete(148), \n",
    "                                  {\"gamma\": 0.99}),\n",
    "            \n",
    "            \"low_level_policy\": (PPOTFPolicy,\n",
    "                                 Box(0, 255, (2048,)),#(len(np.zeros((404, 417, 3)).flatten()),)),\n",
    "                                 MultiDiscrete([69, 99]), \n",
    "                                 {\"gamma\": 0.0}),\n",
    "            \n",
    "#             \"low_level_policy\": (PPOTFPolicy,\n",
    "#                                  Tuple([\n",
    "#                                      lens_env.observation_space,\n",
    "#                                      Discrete(148)\n",
    "#                                  ]), maze.action_space, {\n",
    "#                                      \"gamma\": 0.0\n",
    "#                                  }),\n",
    "        },\n",
    "        \"policy_mapping_fn\": function(policy_mapping_fn),\n",
    "    },\n",
    "    \"framework\": \"tf\",#\"torch\", #if args.torch else \"tf\",\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "}\n",
    "\n",
    "#from ray.rllib.agents.trainer_template import build_trainer\n",
    "\n",
    "# <class 'ray.rllib.agents.trainer_template.MyCustomTrainer'>\n",
    "# MyTrainer = build_trainer(\n",
    "#     name=\"MyCustomTrainer\",\n",
    "#     default_policy=MyTFPolicy)\n",
    "\n",
    "# ray.init()\n",
    "# tune.run(MyTrainer\n",
    "trainer = ppo.PPOTrainer(env= \"BaslerEnv\", config=config)\n",
    "# results = tune.run(\"PPO\", config=config, stop=stop,  \n",
    "#                    restore=r'C:\\Users\\CIG\\Documents\\MATLAB\\ppo_pic\\checkpoint-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 16:28:43,152\tINFO trainable.py:372 -- Restored on 10.16.102.19 from checkpoint: C:\\Users\\CIG\\ray_results\\PPO_noref2\\checkpoint_48\\checkpoint-48\n",
      "2021-04-27 16:28:43,152\tINFO trainable.py:379 -- Current state after restoring: {'_iteration': 48, '_timesteps_total': None, '_time_total': 27332.00341129303, '_episodes_total': 625}\n"
     ]
    }
   ],
   "source": [
    "trainer.restore(r'C:\\Users\\CIG\\ray_results\\PPO_noref2\\checkpoint_48\\checkpoint-48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m 2021-04-27 16:28:46,918\tWARNING deprecation.py:34 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 67] info: {'__all__': False, 'low_level_1': True} {'low_level_1': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 34000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [42  2] info: {'__all__': False, 'low_level_2': True} {'low_level_2': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 32000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 42] info: {'__all__': False, 'low_level_3': True} {'low_level_3': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 91] info: {'__all__': False, 'low_level_4': True} {'low_level_4': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 50] info: {'__all__': False, 'low_level_5': True} {'low_level_5': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 400000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [12  4] info: {'__all__': False, 'low_level_6': True} {'low_level_6': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 42] info: {'__all__': False, 'low_level_7': True} {'low_level_7': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 35000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 42] info: {'__all__': False, 'low_level_8': True} {'low_level_8': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 85] info: {'__all__': False, 'low_level_9': True} {'low_level_9': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 37000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 41] info: {'__all__': False, 'low_level_10': True} {'low_level_10': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 32000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 95] info: {'__all__': False, 'low_level_11': True} {'low_level_11': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 375000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [24 85] info: {'__all__': False, 'low_level_12': True} {'low_level_12': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 330000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [10  9] info: {'__all__': False, 'low_level_13': True} {'low_level_13': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [19 23] info: {'__all__': False, 'low_level_14': True} {'low_level_14': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 39000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 88] info: {'__all__': False, 'low_level_15': True} {'low_level_15': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 37000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 63] info: {'__all__': False, 'low_level_16': True} {'low_level_16': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [46 91] info: {'__all__': False, 'low_level_17': True} {'low_level_17': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [27  4] info: {'__all__': False, 'low_level_18': True} {'low_level_18': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 12] info: {'__all__': False, 'low_level_19': True} {'low_level_19': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 28] info: {'__all__': False, 'low_level_20': True} {'low_level_20': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 55000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 44] info: {'__all__': False, 'low_level_21': True} {'low_level_21': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 55000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: \n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m [47  6] info: {'__all__': False, 'low_level_22': True} {'low_level_22': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 39000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 37] info: {'__all__': False, 'low_level_23': True} {'low_level_23': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 38] info: {'__all__': False, 'low_level_24': True} {'low_level_24': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 37000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 88] info: {'__all__': False, 'low_level_25': True} {'low_level_25': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47  6] info: {'__all__': False, 'low_level_26': True} {'low_level_26': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 55000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [25  0] info: {'__all__': False, 'low_level_27': True} {'low_level_27': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [51  4] info: {'__all__': False, 'low_level_28': True} {'low_level_28': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 39000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 12] info: {'__all__': False, 'low_level_29': True} {'low_level_29': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [22 85] info: {'__all__': False, 'low_level_30': True} {'low_level_30': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 32000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48  4] info: {'__all__': False, 'low_level_31': True} {'low_level_31': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [33 26] info: {'__all__': False, 'low_level_32': True} {'low_level_32': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 34000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 12] info: {'__all__': False, 'low_level_33': True} {'low_level_33': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47  6] info: {'__all__': False, 'low_level_34': True} {'low_level_34': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 74] info: {'__all__': False, 'low_level_35': True} {'low_level_35': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 28] info: {'__all__': False, 'low_level_36': True} {'low_level_36': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [15 42] info: {'__all__': False, 'low_level_37': True} {'low_level_37': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [12  6] info: {'__all__': False, 'low_level_38': True} {'low_level_38': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 39000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 31] info: {'__all__': False, 'low_level_39': True} {'low_level_39': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 32000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 30] info: {'__all__': False, 'low_level_40': True} {'low_level_40': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 42000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 67] info: {'__all__': False, 'low_level_41': True} {'low_level_41': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m c:\\users\\cig\\anaconda3\\envs\\rlautofocus\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\r\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m   self.image = skimage.color.rgb2gray(self.image)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 37] info: {'__all__': True} {'low_level_42': -0.3361, 'high_level_agent': 1}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [31 89] info: {'__all__': False, 'low_level_1': True} {'low_level_1': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 32000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 67] info: {'__all__': False, 'low_level_2': True} {'low_level_2': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 32000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 18] info: {'__all__': False, 'low_level_3': True} {'low_level_3': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47 95] info: {'__all__': False, 'low_level_4': True} {'low_level_4': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47  6] info: {'__all__': False, 'low_level_5': True} {'low_level_5': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [46 57] info: {'__all__': False, 'low_level_6': True} {'low_level_6': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 44000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 58] info: {'__all__': False, 'low_level_7': True} {'low_level_7': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 39000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [48 37] info: {'__all__': False, 'low_level_8': True} {'low_level_8': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [40 21] info: {'__all__': False, 'low_level_9': True} {'low_level_9': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 43000\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m action: [47  9] info: {'__all__': False, 'low_level_10': True} {'low_level_10': 0, 'high_level_agent': -10}\n",
      "\u001b[2m\u001b[36m(pid=23112)\u001b[0m ExposureTimeRaw: 38000\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # Perform one iteration of training the policy with PPO\\n\",\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    print('-------', i, '-------')\n",
    "# #     print(evaluation_results)\n",
    "    if i % 80 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     # Perform one iteration of training the policy with PPO\\n\",\n",
    "#     result = trainer.train()\n",
    "#     print(pretty_print(result))\n",
    "#     print('-------', i, '-------')\n",
    "# # #     print(evaluation_results)\n",
    "#     if i % 80 == 0:\n",
    "#         checkpoint = trainer.save()\n",
    "#         print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stop = {\n",
    "#     \"training_iteration\": 100,\n",
    "#     \"timesteps_total\": 1000,\n",
    "# }\n",
    "\n",
    "# results = tune.run(\"PPO\", stop=stop, config=config, checkpoint_freq=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
