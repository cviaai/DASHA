{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import error\n",
    "from gym.utils import closer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pypylon import pylon\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ctypes import *\n",
    "import glob\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import error\n",
    "from gym.utils import closer\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import A2C, PPO2\n",
    "from gym import spaces\n",
    "from pypylon import pylon\n",
    "import stable_baselines\n",
    "import time\n",
    "from matplotlib import cm\n",
    "from stable_baselines.common.schedules import LinearSchedule\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "env_closer = closer.Closer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorningEnv(gym.Env):\n",
    "    \"\"\"The main OpenAI Gym class. It encapsulates an environment with\n",
    "    arbitrary behind-the-scenes dynamics. An environment can be\n",
    "    partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "        step\n",
    "        reset\n",
    "        render\n",
    "        close\n",
    "        seed\n",
    "    And set the following attributes:\n",
    "        action_space: The Space object corresponding to valid actions\n",
    "        observation_space: The Space object corresponding to valid observations\n",
    "        reward_range: A tuple corresponding to the min and max possible rewards\n",
    "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
    "    The methods are accessed publicly as \"step\", \"reset\", etc...\n",
    "    \n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['rgb_array']}\n",
    "    \n",
    "    def __init__(self, camera,\n",
    "                     Width = 417,\n",
    "                     Height = 404,\n",
    "                     threshold = 85,\n",
    "                     file_path = r'C:\\Users\\CIG\\Documents\\corning_basler_RL'):\n",
    "        \n",
    "        \n",
    "        # Create an instant camera object with the camera device found first.\n",
    "        #self.camera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())\n",
    "        #self.camera.Open()\n",
    "        self.camera = camera\n",
    "        self.camera.AcquisitionFrameRateAbs.Value = 10\n",
    "        self.camera.GainRaw.Value = 36\n",
    "        self.camera.AcquisitionMode.SetValue('Continuous')\n",
    "        self.Width = Width\n",
    "        self.Height = Height\n",
    "        self.camera.Width.Value = Width\n",
    "        self.camera.Height.Value = Height\n",
    "        print(self.camera.Width.GetValue(), self.camera.Height.GetValue())\n",
    "        # code from https://github.com/basler/pypylon/blob/master/samples/opencv.py\n",
    "        img = pylon.PylonImage()\n",
    "        self.converter = pylon.ImageFormatConverter()\n",
    "        # converting to opencv bgr format\n",
    "        self.converter.OutputPixelFormat = pylon.PixelType_BGR8packed\n",
    "        self.converter.OutputBitAlignment = pylon.OutputBitAlignment_MsbAligned\n",
    "        \n",
    "        \n",
    "        # corning lib\n",
    "        self.lib = cdll.LoadLibrary(r\"C:\\Users\\CIG\\Documents\\MATLAB\\ComCasp64.dll\")\n",
    "        #Check if Maxim driver dll is loaded\n",
    "        eCOMCaspErr = getattr(self.lib,'Casp_OpenCOM')\n",
    "        print('eCOMCaspErr:', eCOMCaspErr(), self.lib.Casp_OpenCOM())\n",
    "        \n",
    "        \n",
    "        # yolo\n",
    "        self.model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', \n",
    "                                         force_reload=True, \n",
    "                                         pretrained=True)\n",
    "        \n",
    "        \n",
    "        # environment parameters \n",
    "        # coarse V\n",
    "        # fine V\n",
    "        # \n",
    "        self.action_space = spaces.MultiDiscrete([69, 99, 12])\n",
    "        \n",
    "        # image\n",
    "        self.observation_space = spaces.Box(low=0, \n",
    "                                            high=255, \n",
    "                                            shape=(Height, Width, 3),\n",
    "                                            dtype = np.uint8)\n",
    "        \n",
    "        \n",
    "        self.threshold = threshold\n",
    "        self.file_path = file_path\n",
    "        self.i = 0\n",
    "        self.done_camera = False\n",
    "        time.sleep(1)\n",
    "        \n",
    "    def grab_image(self):\n",
    "        # https://github.com/basler/pypylon/blob/master/samples/opencv.py\n",
    "        self.camera.StartGrabbing()\n",
    "        while 1:\n",
    "            grabResult = self.camera.RetrieveResult(5000, pylon.TimeoutHandling_ThrowException)\n",
    "            if grabResult.GrabSucceeded():\n",
    "                # Access the image data\n",
    "                image = self.converter.Convert(grabResult)\n",
    "                image = image.GetArray()\n",
    "                #print(img[0])\n",
    "                #print('shape:', img.shape)\n",
    "                #plt.imshow(img)\n",
    "                break\n",
    "        self.camera.StopGrabbing()\n",
    "        return image\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial\n",
    "        observation.\n",
    "        Note that this function should not reset the environment's random\n",
    "        number generator(s); random variables in the environment's state should\n",
    "        be sampled independently between multiple calls to `reset()`. In other\n",
    "        words, each call of `reset()` should yield an environment suitable for\n",
    "        a new episode, independent of previous episodes.\n",
    "        Returns:\n",
    "            observation (object): the initial observation.\n",
    "        \"\"\"\n",
    "        self.done_camera = False\n",
    "        \n",
    "        # https://docs.baslerweb.com/features\n",
    "        self.camera.ExposureTimeRaw.Value = 20000\n",
    "        \n",
    "        self.lens_movement(24.0)\n",
    "        \n",
    "        self.i = 0\n",
    "        time.sleep(1)\n",
    "        \n",
    "        obs = self.grab_image().astype(np.uint8)\n",
    "        return obs\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        print(action)\n",
    "        if self.done_camera == False:\n",
    "            action_camera = action[2]\n",
    "            obs_camera, reward_camera, done_camera_ = self.camera_env(action_camera)\n",
    "            self.done_camera = done_camera_\n",
    "            print('Done camera:', self.done_camera)\n",
    "            if self.done_camera == False:\n",
    "                self.i+=1 \n",
    "                return obs_camera, reward_camera, self.done_camera, {}\n",
    "            else:\n",
    "                #print('TRUE')\n",
    "                obs, reward, done = self.lens_env(action)\n",
    "                self.i+=1 \n",
    "                \n",
    "        else:\n",
    "            #print('TRUE2')\n",
    "            obs, reward, done = self.lens_env(action)\n",
    "            self.i+=1 \n",
    "                \n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def lens_env(self, action):\n",
    "        \n",
    "        action1 = 0\n",
    "        action2 = action[1]\n",
    "        if action[0] < 24:\n",
    "            action1 = 24\n",
    "        else:\n",
    "            action1 = action[0]\n",
    "        action = float(\"%s.%s\"% (str(action1), str(action2)))\n",
    "\n",
    "        self.lens_movement(action)\n",
    " \n",
    "        camera_observation = self.grab_image()\n",
    "        prediction_yolo = self.model_yolo(camera_observation)\n",
    "        final_result_yolo = self.find_class(prediction_yolo)\n",
    "        \n",
    "        if final_result_yolo.size == 0:\n",
    "            print('not detected')\n",
    "            foc_value = -1\n",
    "            betta = 0.5\n",
    "            reward = -100-betta*self.i\n",
    "            #reward = -100\n",
    "            \n",
    "        else:\n",
    "            image = Image.fromarray(camera_observation)\n",
    "            image = image.crop((final_result_yolo[0], \n",
    "                             final_result_yolo[1], \n",
    "                             final_result_yolo[2], \n",
    "                             final_result_yolo[3]))\n",
    "            foc_value = self.focus_value(image)\n",
    "            \n",
    "        if foc_value > self.threshold: #or self.i == 20:\n",
    "            done = True\n",
    "            reward = 1\n",
    "            print('------',True, action, self.i, '------')\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 1 - 0.001*(foc_value - self.threshold)**2\n",
    "            #reward = -1\n",
    "            print(False, action)\n",
    "            \n",
    "        im = Image.fromarray(camera_observation)\n",
    "        filename = self.file_path + \"\\img_%s.png\" % (str(action))\n",
    "        im.save(filename)\n",
    "\n",
    "        #print(camera_observation)\n",
    "        camera_observation = camera_observation.astype(np.uint8)\n",
    "            \n",
    "        return camera_observation, reward, done\n",
    "    \n",
    "    def find_class(self, results, obj = 'car'):\n",
    "        len_of_class = len(results.pred)\n",
    "        pred_class = [int(results.pred[0][i].numpy()[5]) for i in range(len(results.pred[0]))]\n",
    "        #list_of_yolo_classes = list(zip(results.names, range(0, len(results.names))))\n",
    "        pred_names = [results.names[i] for i in pred_class]\n",
    "        if obj in pred_names:\n",
    "            index = pred_names.index(obj)\n",
    "            return results.pred[0][index].numpy()\n",
    "        else:\n",
    "            return np.array([])\n",
    "        \n",
    "    def focus_value(self, im):\n",
    "        # Calculate the gradient\n",
    "        sobelx = cv2.Sobel(np.float32(im), cv2.CV_64F ,1 , 0, ksize=5)\n",
    "        sobely = cv2.Sobel(np.float32(im),cv2.CV_64F,0,1,ksize=5)\n",
    "\n",
    "        abs_sobel_x = cv2.convertScaleAbs(sobelx) # converting back to uint8\n",
    "        abs_sobel_y = cv2.convertScaleAbs(sobely)\n",
    "        #print(abs_sobel_x )\n",
    "\n",
    "        # Combine the two gradients with equal weight\n",
    "        dst = cv2.addWeighted(abs_sobel_x,0.5,abs_sobel_y,0.5,0)\n",
    "        #print(dst)\n",
    "\n",
    "        # Calculate the average gradient for the image\n",
    "        # I convert it to a numpy array for ease of calculation\n",
    "        return pl.asarray(dst).mean()\n",
    "    \n",
    "    def lens_movement(self, action):\n",
    "        x = c_double(action)\n",
    "        self.lib.Casp_SetFocusVoltage(x)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    def camera_env(self, action_camera):\n",
    "        \n",
    "        if action_camera == 0:\n",
    "            ExposureTimeRaw = random.randint(1e3,5e3)\n",
    "        if action_camera == 1:\n",
    "            ExposureTimeRaw = random.randint(6e3,9e3)\n",
    "        if action_camera == 2:\n",
    "            ExposureTimeRaw = random.randint(10e3,25e3)\n",
    "        if action_camera == 3:\n",
    "            ExposureTimeRaw = random.randint(26e3,50e3)\n",
    "        if action_camera == 4:\n",
    "            ExposureTimeRaw = random.randint(51e3,75e3)\n",
    "        if action_camera == 5:\n",
    "            ExposureTimeRaw = random.randint(76e3,99e3)\n",
    "        if action_camera == 6:\n",
    "            ExposureTimeRaw = random.randint(100e3,250e3)\n",
    "        if action_camera == 7:\n",
    "            ExposureTimeRaw = random.randint(260e3,500e3)\n",
    "        if action_camera == 8:\n",
    "            ExposureTimeRaw = random.randint(510e3,700e3)\n",
    "        if action_camera == 9:\n",
    "            ExposureTimeRaw = random.randint(710e3,999e3)\n",
    "        if action_camera == 10:\n",
    "            ExposureTimeRaw = random.randint(100e4,250e4)\n",
    "        if action_camera == 11:\n",
    "            ExposureTimeRaw = random.randint(260e4,500e4)\n",
    "        if action_camera == 12:\n",
    "            ExposureTimeRaw = random.randint(510e4,750e4)\n",
    "#         if action_camera == 13:\n",
    "#             ExposureTimeRaw = random.randint(760e4,900e4)\n",
    "            \n",
    "        print('ExposureTimeRaw:', ExposureTimeRaw)\n",
    "        self.lens_movement(46.0)\n",
    "        self.camera.ExposureTimeRaw.Value = ExposureTimeRaw\n",
    "        time.sleep(5)\n",
    "        obs_camera = self.grab_image().astype(np.uint8)\n",
    "        \n",
    "        im = Image.fromarray(obs_camera)\n",
    "        filename = self.file_path + \"\\img_ExTime_%s.png\" % (str(ExposureTimeRaw))\n",
    "        im.save(filename)\n",
    "        \n",
    "        hist , bin_edges = np.histogram(np.array(obs_camera).ravel())\n",
    "        #reward = np.sum(bin_edges) - 250\n",
    "        max_value_bin = bin_edges[np.argmax(hist)]\n",
    "        if max_value_bin > 50 and max_value_bin < 170:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        #hist, _ = np.histogram(np.array(obs_camera).ravel(), 255,[0,255])\n",
    "        \n",
    "        #array = np.concatenate((hist, bin_edges))\n",
    "        #array_null = np.zeros((len(np.array(obs_camera).ravel()) - len(array), ))\n",
    "        #obs_camera = np.concatenate((array, array_null)).reshape((self.Height, self.Width, 3))\n",
    "        \n",
    "        if reward == 1:\n",
    "            done_camera = True\n",
    "            reward_camera = 1\n",
    "        else:\n",
    "            done_camera = False\n",
    "            reward_camera = reward\n",
    "            \n",
    "        return obs_camera, reward_camera, done_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instant camera object with the camera device found first.\n",
    "camera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())\n",
    "camera.Open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417 404\n",
      "eCOMCaspErr: 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\CIG/.cache\\torch\\hub\\master.zip\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPS\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoShape... \n"
     ]
    }
   ],
   "source": [
    "env = CorningEnv(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines.common.env_checker import check_env\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "#check_env(env)\n",
    "#env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "# model = PPO2(MlpPolicy, env, \n",
    "#              #learning_rate = sched_LR.value,\n",
    "#              verbose=1, \n",
    "#              tensorboard_log = '')\n",
    "\n",
    "# Load the trained agent\n",
    "model = PPO2.load(\"model_corning_basler\", env=DummyVecEnv([lambda: env]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 85 10]\n",
      "ExposureTimeRaw: 2297003\n",
      "Done camera: False\n",
      "[42 37  8]\n",
      "ExposureTimeRaw: 677919\n",
      "Done camera: False\n",
      "[38 73  1]\n",
      "ExposureTimeRaw: 6571\n",
      "Done camera: False\n",
      "[33 37  4]\n",
      "ExposureTimeRaw: 54893\n",
      "Done camera: False\n",
      "[13 79  4]\n",
      "ExposureTimeRaw: 51658\n",
      "Done camera: False\n",
      "[15 18  3]\n",
      "ExposureTimeRaw: 33861\n",
      "Done camera: True\n",
      "not detected\n",
      "False 24.18\n",
      "[ 8 20  5]\n",
      "not detected\n",
      "False 24.2\n",
      "[39 53  6]\n",
      "not detected\n",
      "False 39.53\n",
      "[54 90  8]\n",
      "not detected\n",
      "False 54.9\n",
      "[1 1 4]\n",
      "not detected\n",
      "False 24.1\n",
      "[43 55  6]\n",
      "not detected\n",
      "False 43.55\n",
      "[ 0 36  2]\n",
      "not detected\n",
      "False 24.36\n",
      "[50 17  8]\n",
      "not detected\n",
      "False 50.17\n",
      "[21 93  2]\n",
      "not detected\n",
      "False 24.93\n",
      "[ 6 25  6]\n",
      "not detected\n",
      "False 24.25\n",
      "[28 16  7]\n",
      "not detected\n",
      "False 28.16\n",
      "[48  2 10]\n",
      "------ True 48.2 16 ------\n",
      "[39 17  8]\n",
      "ExposureTimeRaw: 660825\n",
      "Done camera: False\n",
      "[24 65  8]\n",
      "ExposureTimeRaw: 532629\n",
      "Done camera: False\n",
      "[42 86  1]\n",
      "ExposureTimeRaw: 6738\n",
      "Done camera: False\n",
      "[26 27  0]\n",
      "ExposureTimeRaw: 1472\n",
      "Done camera: False\n",
      "[39 83  7]\n",
      "ExposureTimeRaw: 407364\n",
      "Done camera: False\n",
      "[17 32  8]\n",
      "ExposureTimeRaw: 556324\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=3000, tb_log_name=\"corning_basler_att1\", reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model_corning_basler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=C:\\\\Users\\\\CIG\\\\Documents\\\\MATLAB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
